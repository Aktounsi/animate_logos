{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Hierarchical Generative Network\n",
    "Hierarchical generative network is trained to produce latent vector z from every path of a logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"src/preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsvg.config import _Config\n",
    "from deepsvg import utils\n",
    "from deepsvg.utils import Stats, TrainVars, Timer\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg: _Config, model_name, experiment_name=\"\", log_dir=\"./logs\", debug=False, resume=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Parameters\")\n",
    "    cfg.print_params()\n",
    "\n",
    "    print(\"Loading dataset\")\n",
    "    dataset_load_function = importlib.import_module(cfg.dataloader_module).load_dataset\n",
    "    dataset = dataset_load_function(cfg)\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, drop_last=True,\n",
    "                            num_workers=cfg.loader_num_workers, collate_fn=cfg.collate_fn)\n",
    "    model = cfg.make_model().to(device)\n",
    "\n",
    "    if cfg.pretrained_path is not None:\n",
    "        print(f\"Loading pretrained model {cfg.pretrained_path}\")\n",
    "        utils.load_model(cfg.pretrained_path, model)\n",
    "\n",
    "    stats = Stats(num_steps=cfg.num_steps, num_epochs=cfg.num_epochs, steps_per_epoch=len(dataloader),\n",
    "                  stats_to_print=cfg.stats_to_print)\n",
    "    train_vars = TrainVars()\n",
    "    timer = Timer()\n",
    "\n",
    "    stats.num_parameters = utils.count_parameters(model)\n",
    "    print(f\"#Parameters: {stats.num_parameters:,}\")\n",
    "\n",
    "    # Summary Writer\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    experiment_identifier = f\"{model_name}_{experiment_name}_{current_time}\"\n",
    "\n",
    "    summary_writer = SummaryWriter(os.path.join(log_dir, \"tensorboard\", \"debug\" if debug else \"full\", experiment_identifier))\n",
    "    checkpoint_dir = os.path.join(log_dir, \"models\", model_name, experiment_name)\n",
    "    visualization_dir = os.path.join(log_dir, \"visualization\", model_name, experiment_name)\n",
    "\n",
    "    #cfg.set_train_vars(train_vars, dataloader)\n",
    "\n",
    "    # Optimizer, lr & warmup schedulers\n",
    "    optimizers = cfg.make_optimizers(model)\n",
    "    scheduler_lrs = cfg.make_schedulers(optimizers, epoch_size=len(dataloader))\n",
    "    scheduler_warmups = cfg.make_warmup_schedulers(optimizers, scheduler_lrs)\n",
    "\n",
    "    loss_fns = [l.to(device) for l in cfg.make_losses()]\n",
    "\n",
    "    if resume:\n",
    "        ckpt_exists = utils.load_ckpt_list(checkpoint_dir, model, None, optimizers, scheduler_lrs, scheduler_warmups, stats, train_vars)\n",
    "\n",
    "    if resume and ckpt_exists:\n",
    "        print(f\"Resuming model at epoch {stats.epoch+1}\")\n",
    "        stats.num_steps = cfg.num_epochs * len(dataloader)\n",
    "    else:\n",
    "        # Run a single forward pass on the single-device model for initialization of some modules\n",
    "        single_foward_dataloader = DataLoader(dataset, batch_size=cfg.batch_size // cfg.num_gpus, shuffle=True, drop_last=True, num_workers=cfg.loader_num_workers, collate_fn=cfg.collate_fn)\n",
    "        data = next(iter(single_foward_dataloader))\n",
    "        model_args, params_dict = [data[arg].to(device) for arg in cfg.model_args], cfg.get_params(0, 0)\n",
    "        model(*model_args, params=params_dict)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    epoch_range = utils.infinite_range(stats.epoch) if cfg.num_epochs is None else range(stats.epoch, cfg.num_epochs)\n",
    "    for epoch in epoch_range:\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "        for n_iter, data in enumerate(dataloader):\n",
    "            step = n_iter + epoch * len(dataloader)\n",
    "\n",
    "            if cfg.num_steps is not None and step > cfg.num_steps:\n",
    "                return\n",
    "\n",
    "            model.train()\n",
    "            model_args = [data[arg].to(device) for arg in cfg.model_args]\n",
    "            labels = data[\"label\"].to(device) if \"label\" in data else None\n",
    "            params_dict, weights_dict = cfg.get_params(step, epoch), cfg.get_weights(step, epoch)\n",
    "\n",
    "            for i, (loss_fn, optimizer, scheduler_lr, scheduler_warmup, optimizer_start) in enumerate(zip(loss_fns, optimizers, scheduler_lrs, scheduler_warmups, cfg.optimizer_starts), 1):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(*model_args, params=params_dict)\n",
    "                loss_dict = loss_fn(output, labels, weights=weights_dict)\n",
    "\n",
    "                if step >= optimizer_start:\n",
    "                    loss_dict[\"loss\"].backward()\n",
    "                    if cfg.grad_clip is not None:\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    if scheduler_lr is not None:\n",
    "                        scheduler_lr.step()\n",
    "                    if scheduler_warmup is not None:\n",
    "                        scheduler_warmup.step()\n",
    "\n",
    "                stats.update_stats_to_print(\"train\", loss_dict.keys())\n",
    "                stats.update(\"train\", step, epoch, {\n",
    "                    (\"lr\" if i == 1 else f\"lr_{i}\"): optimizer.param_groups[0]['lr'],\n",
    "                    **loss_dict\n",
    "                })\n",
    "\n",
    "            stats.update(\"train\", step, epoch, {\n",
    "                **weights_dict,\n",
    "                \"time\": timer.get_elapsed_time()\n",
    "            })\n",
    "\n",
    "            if step % cfg.log_every == 0 and step > 0:\n",
    "                print(stats.get_summary(\"train\"))\n",
    "                stats.write_tensorboard(summary_writer, \"train\")\n",
    "                summary_writer.flush()\n",
    "\n",
    "            if step % cfg.val_every == 0 and step > 0:\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Visualization\n",
    "                    output = None\n",
    "                    cfg.visualize(model, output, train_vars, step, epoch, summary_writer, visualization_dir)\n",
    "\n",
    "                timer.reset()\n",
    "\n",
    "            if not debug and step % cfg.ckpt_every == 0 and step > 0:\n",
    "                utils.save_ckpt_list(checkpoint_dir, model, cfg, optimizers, scheduler_lrs, scheduler_warmups, stats, train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--resume'], dest='resume', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='DeepSVG Trainer')\n",
    "parser.add_argument(\"--config-module\", type=str, required=True)\n",
    "parser.add_argument(\"--log-dir\", type=str, default=\"./logs\")\n",
    "parser.add_argument(\"--debug\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--config-module configs.deepsvg.hierarchical_ordered --log-dir ./logs\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "  batch_size = 120\n",
      "  ckpt_every = 1000\n",
      "  collate_fn = None\n",
      "  data_dir = ./data/svgs_tensors/\n",
      "  dataloader_module = deepsvg.svgtensor_dataset\n",
      "  filter_category = None\n",
      "  filter_labels = None\n",
      "  filter_platform = None\n",
      "  filter_uni = None\n",
      "  grad_clip = 1.0\n",
      "  learning_rate = 0.002\n",
      "  loader_num_workers = 8\n",
      "  log_every = 20\n",
      "  max_num_groups = 8\n",
      "  max_seq_len = 30\n",
      "  max_total_len = 50\n",
      "  meta_filepath = ./data/svg_meta.csv\n",
      "  model_args = ['commands', 'args', 'commands', 'args']\n",
      "  model_cfg = <configs.deepsvg.hierarchical_ordered.ModelConfig object at 0x0000029B3B73FE88>\n",
      "  nb_augmentations = 1\n",
      "  num_epochs = 50\n",
      "  num_gpus = 2\n",
      "  num_steps = None\n",
      "  optimizer_starts = [0]\n",
      "  pretrained_path = None\n",
      "  stats_to_print = {'train': ['lr', 'time']}\n",
      "  train_ratio = 1.0\n",
      "  val_every = 2000\n",
      "  warmup_steps = 500\n",
      "Loading dataset\n",
      "#Parameters: 10,304,596\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 126, in __getitem__\n    return self.get(idx, self.model_args)\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 162, in get\n    return self.get_data(t_sep, fillings, model_args=model_args, label=label)\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 172, in get_data\n    t_sep.extend([torch.empty(0, 14)] * pad_len)\nAttributeError: 'Tensor' object has no attribute 'extend'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2c8b4675642c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-4f509069d6a1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cfg, model_name, experiment_name, log_dir, debug, resume)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Run a single forward pass on the single-device model for initialization of some modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0msingle_foward_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader_num_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_foward_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 126, in __getitem__\n    return self.get(idx, self.model_args)\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 162, in get\n    return self.get_data(t_sep, fillings, model_args=model_args, label=label)\n  File \"C:\\Users\\sarah\\PycharmProjects\\animate_logos\\src\\preprocessing\\deepsvg\\svgtensor_dataset.py\", line 172, in get_data\n    t_sep.extend([torch.empty(0, 14)] * pad_len)\nAttributeError: 'Tensor' object has no attribute 'extend'\n"
     ]
    }
   ],
   "source": [
    "cfg = importlib.import_module(args.config_module).Config()\n",
    "model_name, experiment_name = args.config_module.split(\".\")[-2:]\n",
    "\n",
    "train(cfg, model_name, experiment_name, log_dir=args.log_dir, debug=args.debug, resume=args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
